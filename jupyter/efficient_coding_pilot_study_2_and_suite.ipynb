{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35fee16",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import logistic\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import arviz as az\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c25f22e4",
   "metadata": {},
   "source": [
    "# Get data for all emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee77759c",
   "metadata": {},
   "source": [
    "# load rating data from pilot study 2\n",
    "all_data = pd.read_csv(\"/Users/jadeserfaty/Library/Mobile Documents/com~apple~CloudDocs/code/lido/introspection_task/pilot_study_2_data/pilot_study_2_both_parts_rating_data/rating_data_anxiety.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13271907",
   "metadata": {},
   "source": [
    "all_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "522bea18",
   "metadata": {},
   "source": [
    "## Choose one emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93a0b91",
   "metadata": {},
   "source": [
    "# so let's start by testing out anxiety for example\n",
    "emotion_focus = \"anxiety\"\n",
    "all_data_emo = all_data[all_data[\"emotionName\"] == emotion_focus].copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9b84b9f8",
   "metadata": {},
   "source": [
    "# Define functions for bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082c4c27",
   "metadata": {},
   "source": [
    "def f(v, mu, s):\n",
    "    return 0.5 * (1 + np.tanh((v - mu) / (2 * s)))\n",
    "\n",
    "def f_theano(v,mu,s):\n",
    "    return 0.5 * (1 + tt.tanh((v - mu) / (2 * s)))\n",
    "\n",
    "def f_inv(v_tilde, mu, s):\n",
    "    v_tilde = np.clip(v_tilde, 1e-8, 1 - 1e-8) # find another way of doing this but basically will always need to clip because extremeties don't work in this model\n",
    "    result = mu + s * np.log(v_tilde / (1 - v_tilde))\n",
    "    return result\n",
    "\n",
    "# The first derivative of the inverse CDF\n",
    "def phi_prime(v_tilde, s):\n",
    "    return s / (v_tilde * (1 - v_tilde))\n",
    "\n",
    "# The second derivative of the inverse CDF\n",
    "def phi_double_prime(v_tilde, s):\n",
    "    return s * (2 * v_tilde - 1) / (v_tilde ** 2 * (1 - v_tilde) ** 2)\n",
    "\n",
    "def f_prime_theano(v, mu, s):\n",
    "    p = f_theano(v, mu, s)\n",
    "    return p * (1 - p)\n",
    "\n",
    "def g(v_hat):\n",
    "    return 1 / (1 + np.exp(-v_hat))\n",
    "\n",
    "def g_inv_theano(v):\n",
    "    return tt.log(v / (1 - v))\n",
    "\n",
    "def g_inv_prime_theano(v):\n",
    "    return 1 / (v * (1 - v))\n",
    "\n",
    "# Define the sech function using theano\n",
    "def sech(x):\n",
    "    return 2 / (tt.exp(x) + tt.exp(-x))\n",
    "\n",
    "# Define the log probability function for the logistic distribution\n",
    "def logp_logistic(v, mu, s):\n",
    "    # logistic PDF transformed to log-probability\n",
    "    return tt.log(sech((v - mu) / (2 * s))**2 / (4 * s))\n",
    "\n",
    "# Adaptation of functions using theanos, with numpy for visualization of prior distribution\n",
    "def sech_bis(x):\n",
    "    return 2 / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "def logp_logistic_bis(v, mu, s):\n",
    "    return np.log(sech_bis((v - mu) / (2 * s))**2 / (4 * s))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d2553048",
   "metadata": {},
   "source": [
    "# Fit Bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5dda219",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "delta = 0.05 "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec07d57",
   "metadata": {},
   "source": [
    "all_participants = np.unique(all_data_emo[\"subject_id\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00194d",
   "metadata": {},
   "source": [
    "posterior_distributions_all_participants = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f62d4e40",
   "metadata": {},
   "source": [
    "participant_no_convergence = [\"yv4dL8QeFhgdGWHbmE7mGTBeWfk2\", \"yG2DqQEFfTUa8ia9kV3TXMS3Xvh2\", \"pEws4YaPQLhWeNFNtVVwDIx4vvM2\", \"il7v5DrkOOUZT4zsi6xGPE74lhC3\", \"aM1CxLBZisfaOXevMwqsUy7nQE23\", \"FcfUH1l7WOdZFIhDl2RNobD9bJq1\", \"HrVEfoH7KFTgCClM7OFCQ2S0Azz1\"]\n",
    "print(len(participant_no_convergence))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce1f05be",
   "metadata": {},
   "source": [
    "# TODO: modify code such that I save for each participant their behavioral normalized data too"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1591e58d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for participant in all_participants: \n",
    "    if participant not in posterior_distributions_all_participants.keys() and participant not in [\"yv4dL8QeFhgdGWHbmE7mGTBeWfk2\", \"yG2DqQEFfTUa8ia9kV3TXMS3Xvh2\", \"pEws4YaPQLhWeNFNtVVwDIx4vvM2\", \"il7v5DrkOOUZT4zsi6xGPE74lhC3\", \"aM1CxLBZisfaOXevMwqsUy7nQE23\", \"FcfUH1l7WOdZFIhDl2RNobD9bJq1\", \"HrVEfoH7KFTgCClM7OFCQ2S0Azz1\"]:\n",
    "        print(participant)\n",
    "        # we're fitting the model for each emotion and for each participant\n",
    "        participant_emo = all_data_emo[all_data_emo[\"subject_id\"] == participant].copy()\n",
    "\n",
    "        # Extract the number of videos \n",
    "        num_videos = len(participant_emo)\n",
    "\n",
    "        # Define the parameters for the prior distribution\n",
    "        # Normalize the 'average_rating' to a 0-1 scale\n",
    "        participant_emo.loc[:, 'normalized_rating_phase1'] = (participant_emo['rating_phase1'] - 1) / (7 - 1)\n",
    "        participant_emo.loc[:, 'normalized_rating_phase2'] = (participant_emo['rating_phase2'] - 1) / (7 - 1)\n",
    "\n",
    "        # Compute the average of the normalized ratings\n",
    "        participant_emo.loc[:, 'normalized_average_rating'] = participant_emo.loc[:, ['normalized_rating_phase1', 'normalized_rating_phase2']].mean(axis=1)\n",
    "        participant_emo.loc[:, 'normalized_variance_rating'] = participant_emo.loc[:, ['normalized_rating_phase1', 'normalized_rating_phase2']].std(axis=1)\n",
    "\n",
    "        # Plot of 'normalized_average_rating' so normalized average ratings on anxiety for participant of focus \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(participant_emo['normalized_average_rating'], bins=10, color='skyblue', edgecolor='black')\n",
    "        plt.title('Histogram of Normalized Average Ratings')\n",
    "        plt.xlabel('Normalized Average Rating')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate new parameters on the normalized scale\n",
    "        mu_empirical = participant_emo['normalized_average_rating'].mean()\n",
    "        s_empirical = participant_emo['normalized_average_rating'].std()\n",
    "\n",
    "        print(\"Estimated Prior Mean:\", mu_empirical)\n",
    "        print(\"Estimated Prior Standard Deviation:\", s_empirical)\n",
    "\n",
    "        # Plot priors for visualization \n",
    "        # Define the range for v\n",
    "        v_values = np.linspace(0, 1, 100)\n",
    "\n",
    "        # Plot Uniform distribution for v\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(v_values, np.ones_like(v_values), label='Uniform(0,1)')\n",
    "        plt.title('Uniform Prior for v')\n",
    "        plt.xlabel('v')\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot HalfNormal distribution for standard deviations\n",
    "        sigma_values = np.linspace(0, 3 * s_empirical, 100)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(sigma_values, (np.sqrt(2/np.pi) * np.exp(-0.5 * (sigma_values/s_empirical)**2)), label=f'HalfNormal({np.round(s_empirical,3)})')\n",
    "        plt.title('HalfNormal Prior for Standard Deviations')\n",
    "        plt.xlabel('sigma')\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        logistic_prior = logp_logistic_bis(v_values, mu_empirical, s_empirical)\n",
    "        plt.plot(v_values, np.exp(logistic_prior), label=f'Logistic Prior\\nmu={np.round(mu_empirical,3)}, s={np.round(s_empirical,3)}')\n",
    "        plt.title('Custom Logistic Prior')\n",
    "        plt.xlabel('v')\n",
    "        plt.ylabel('Log Probability Density')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            # Bayesian model Setup\n",
    "            with pm.Model() as model:\n",
    "                # Prior distributions for parameters\n",
    "                mu = pm.Normal('mu', mu=mu_empirical, sigma=s_empirical)\n",
    "                s = pm.HalfNormal('s', sigma=s_empirical)\n",
    "                sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "                sigma_ext = pm.HalfNormal('sigma_ext', sigma=1)\n",
    "\n",
    "                # Observations\n",
    "                # Convert simulated_noisy_ratings from a NumPy array to a Theano tensor by using it as observed data\n",
    "                observed_noisy_ratings = pm.Data(\"observed_noisy_ratings\", participant_emo['normalized_average_rating'])\n",
    "\n",
    "                # Define the latent variable v with a logistic prior\n",
    "                v = pm.Uniform('v', 0, 1, shape=num_videos)  # Starting with a Uniform prior for simplicity\n",
    "                # Add the custom logistic prior using a Potential\n",
    "                pm.Potential('v_prior_potential', logp_logistic(v, mu, s))\n",
    "\n",
    "                # Likelihood function components\n",
    "                phi_prime_val = phi_prime(v, s)\n",
    "                phi_double_prime_val = phi_double_prime(v, s)\n",
    "                F_prime_v0_val = f_prime_theano(v, mu, s)\n",
    "                g_inv_prime_val = g_inv_prime_theano(observed_noisy_ratings)\n",
    "\n",
    "                # The mean and variance for the likelihood normal distribution\n",
    "                mean_likelihood = v + phi_double_prime_val * sigma**2\n",
    "                sd_likelihood = tt.sqrt((phi_prime_val**2) * sigma**2 + sigma_ext)\n",
    "\n",
    "                # Define the observed likelihood\n",
    "                observed = pm.Normal('observed',\n",
    "                                     mu=mean_likelihood,\n",
    "                                     sigma=sd_likelihood,\n",
    "                                     observed=observed_noisy_ratings)\n",
    "\n",
    "                # Include the multiplicative factors in the likelihood using a Potential\n",
    "                # The Potential object is used to add unnormalized log-probability terms to the model's joint density. \n",
    "                # These terms are often used for including domain-specific constraints or additional likelihood terms \n",
    "                # that don't conform to standard probability distributions.\n",
    "                # Ensure that F_prime_v0 and g_inv_prime are theano tensors to enable automatic differentiation\n",
    "                likelihood_factors = pm.Potential('likelihood_factors',\n",
    "                                                  tt.log(F_prime_v0_val) +\n",
    "                                                  tt.log(g_inv_prime_val))\n",
    "\n",
    "\n",
    "                # Sampling from the posterior\n",
    "                trace = pm.sample(2000, tune=1000, cores=2, target_accept=0.95, return_inferencedata=False)\n",
    "\n",
    "                # Check the convergence\n",
    "                # visualize the trace of each parameter to check for convergence visually\n",
    "                pm.plot_trace(trace)\n",
    "                # provide a statistical summary of the trace (mean, standard deviation, quantiles, etc.). \n",
    "                summary_stats = pm.summary(trace).round(2)\n",
    "\n",
    "                # Save posterior distributions for particpant\n",
    "                posterior_distributions_all_participants[participant] = {}\n",
    "                posterior_distributions_all_participants[participant][\"summary_stats\"] = summary_stats\n",
    "\n",
    "                # The Gelman-Rubin diagnostic (or Rhat)\n",
    "                # Assesses the convergence of the MCMC chains by comparing the variance between chains to the variance within chains. \n",
    "                # A value close to 1 (typically, 1.01 or lower) indicates good convergence.\n",
    "                gelman_rubin = az.rhat(trace)\n",
    "                print(\"Gelman-Rubin Diagnostic:\")\n",
    "                print(gelman_rubin)\n",
    "\n",
    "                # Effective Sample Size\n",
    "                # Measures the number of independent-like samples in the chain. \n",
    "                # Higher values are better, as they indicate more information and less autocorrelation.\n",
    "                effective_sample_size = az.ess(trace)\n",
    "                print(\"Effective Sample Size:\")\n",
    "                print(effective_sample_size)\n",
    "\n",
    "                # Convergence Diagnostics\n",
    "                # checks for autocorrelation within each parameter's trace. \n",
    "                # High autocorrelation can indicate slow mixing and might suggest that more tuning \n",
    "                # or a higher number of iterations are needed during sampling.\n",
    "                az.plot_autocorr(trace)\n",
    "\n",
    "                # Posterior Predictive Checks\n",
    "                # Generates new data points from the posterior distribution of the model parameters to assess \n",
    "                # how well the model could have predicted the observed data. \n",
    "                # This is crucial for validating the model's predictive power.\n",
    "                ppc = pm.sample_posterior_predictive(trace, var_names=[\"observed\"])\n",
    "\n",
    "                # We'll plot this once we do the adjustments for the predictions\n",
    "    #             az.plot_ppc(az.from_pymc3(posterior_predictive=adjusted_ppc, model=model))\n",
    "\n",
    "                # Extract the predicted data\n",
    "                predicted_data = ppc['observed']\n",
    "                posterior_distributions_all_participants[participant][\"predicted_data\"] = predicted_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6125fef2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(len(posterior_distributions_all_participants))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55faf487",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for participant in posterior_distributions_all_participants.keys():\n",
    "    \n",
    "    participant_emo = all_data_emo[all_data_emo[\"subject_id\"] == participant].copy()\n",
    "\n",
    "    # Define the parameters for the prior distribution\n",
    "    # Normalize the 'average_rating' to a 0-1 scale\n",
    "    participant_emo.loc[:, 'normalized_rating_phase1'] = (participant_emo['rating_phase1'] - 1) / (7 - 1)\n",
    "    participant_emo.loc[:, 'normalized_rating_phase2'] = (participant_emo['rating_phase2'] - 1) / (7 - 1)\n",
    "\n",
    "    # Compute the average of the normalized ratings\n",
    "    participant_emo.loc[:, 'normalized_average_rating'] = participant_emo.loc[:, ['normalized_rating_phase1', 'normalized_rating_phase2']].mean(axis=1)     \n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.hist(participant_emo[\"normalized_average_rating\"], label= \"behavioral\")\n",
    "    plt.hist(posterior_distributions_all_participants[participant][\"summary_stats\"]['mean'][4:], label = \"posterior\")\n",
    "    plt.title(participant)\n",
    "    plt.legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c2145b39",
   "metadata": {},
   "source": [
    "# Modeling choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "659f87c2",
   "metadata": {},
   "source": [
    "# Define the integrands for mu and sigma^2 calculations\n",
    "def mu_integrand(v, v0, sigma, sigma_ext ,s):\n",
    "    phi_prime_val = phi_prime(v , s)\n",
    "    phi_double_prime_val =  phi_double_prime(v, s)\n",
    "    density = norm.pdf(v, loc=v0 + phi_double_prime_val * sigma**2, scale=np.sqrt(phi_prime_val**2 * sigma**2 + sigma_ext))\n",
    "    return g(v) * density\n",
    "\n",
    "def sigma_integrand(v, v0, mu_v_hat, sigma, sigma_ext, s):\n",
    "    phi_prime_val = phi_prime(v, s)\n",
    "    phi_double_prime_val =  phi_double_prime(v, s)\n",
    "    density = norm.pdf(v, loc=v0 + phi_double_prime_val * sigma**2, scale=np.sqrt(phi_prime_val**2 * sigma**2 + sigma_ext))\n",
    "    return (g(v) - mu_v_hat)**2 * density\n",
    "\n",
    "# Calculate mu_v_hat and sigma_v_hat^2 for a given v0\n",
    "def calculate_mu_sigma(v0, sigma, sigma_ext, s):\n",
    "    # Calculate mu_v_hat\n",
    "    mu_v_hat, _ = quad(mu_integrand, -np.inf, np.inf, args=(v0, sigma, sigma_ext, s))\n",
    "    \n",
    "    # Calculate sigma_v_hat^2 using mu_v_hat\n",
    "    sigma_v_hat_squared, _ = quad(sigma_integrand, -np.inf, np.inf, args=(v0, mu_v_hat, sigma, sigma_ext, s))\n",
    "    \n",
    "    return mu_v_hat, sigma_v_hat_squared"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cce19d08",
   "metadata": {},
   "source": [
    "# Define the sequence of subjective values v_hat on the rating scale (1 to 7 with step 0.01)\n",
    "v_hat_sequence = np.arange(1, 7.01, 0.01)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f54c93c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Placeholder for storing mu_v_hat and var_v_hat\n",
    "mu_v_hat_values = np.zeros_like(v_hat_sequence)\n",
    "var_v_hat_values = np.zeros_like(v_hat_sequence)\n",
    "\n",
    "# Calculate mu_v_hat and var_v_hat for each v_hat in the sequence\n",
    "for i, v_hat in enumerate(v_hat_sequence):\n",
    "    mu_v_hat, var_v_hat = calculate_mu_sigma(v0 = v_hat, sigma = sigma_estimate, sigma_ext = sigma_ext_estimate, s = s_estimate)\n",
    "    mu_v_hat_values[i] = mu_v_hat\n",
    "    var_v_hat_values[i] = var_v_hat"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28078ac6",
   "metadata": {},
   "source": [
    "var_v_hat_values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ac4fecf",
   "metadata": {},
   "source": [
    "# Function to calculate choice consistency for a given pair of stimuli\n",
    "def choice_consistency(mu_v_hat, var_v_hat, mu_v_hat_plus_delta, var_v_hat_plus_delta, mu_v_hat_min_delta, var_v_hat_min_delta):\n",
    "    # Calculating choice consistency using the provided formula\n",
    "    # This part involves calculating the cumulative distribution function values\n",
    "    # for the normal distribution based on the differences in expected values and variances.\n",
    "    consistency = 0.5 * (norm.cdf((mu_v_hat_plus_delta - mu_v_hat) / np.sqrt(var_v_hat_plus_delta - var_v_hat)) +\n",
    "                         norm.cdf((mu_v_hat_min_delta - mu_v_hat) / np.sqrt(var_v_hat_min_delta - var_v_hat)))\n",
    "    return consistency"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0a04a",
   "metadata": {},
   "source": [
    "# Example calculation for a pair of stimuli\n",
    "# This is a simplified example; in practice, you would apply this to all pairs of stimuli in your choice task.\n",
    "v_hat_1, v_hat_2 = 3, 5  # Example subjective values for two stimuli\n",
    "v_hat_1_plus_delta, v_hat_2_plus_delta = v_hat_1 + delta, v_hat_2 + delta\n",
    "v_hat_1_min_delta, v_hat_2_min_delta = v_hat_1 - delta, v_hat_2 - delta\n",
    "\n",
    "mu_v_hat_1, var_v_hat_1 = calculate_mu_var(v_hat_1, lambda_param, sigma, sigma_ext)\n",
    "mu_v_hat_2, var_v_hat_2 = calculate_mu_var(v_hat_2, lambda_param, sigma, sigma_ext)\n",
    "\n",
    "mu_v_hat_1_plus_delta, var_v_hat_1_plus_delta = calculate_mu_var(v_hat_1_plus_delta, lambda_param, sigma, sigma_ext)\n",
    "mu_v_hat_2_plus_delta, var_v_hat_2_plus_delta = calculate_mu_var(v_hat_2_plus_delta, lambda_param, sigma, sigma_ext)\n",
    "\n",
    "mu_v_hat_1_min_delta, var_v_hat_1_min_delta = calculate_mu_var(v_hat_1_min_delta, lambda_param, sigma, sigma_ext)\n",
    "mu_v_hat_2_min_delta, var_v_hat_2_min_delta = calculate_mu_var(v_hat_2_min_delta, lambda_param, sigma, sigma_ext)\n",
    "\n",
    "\n",
    "consistency_1 = choice_consistency(mu_v_hat_1, var_v_hat_1, mu_v_hat_1_plus_delta, var_v_hat_1_plus_delta, mu_v_hat_1_min_delta, var_v_hat_1_min_delta)\n",
    "consistency_2 = choice_consistency(mu_v_hat_2, var_v_hat_2, mu_v_hat_2_plus_delta, var_v_hat_2_plus_delta, mu_v_hat_2_min_delta, var_v_hat_2_min_delta)\n",
    "\n",
    "print(f\"Choice consistency for stimulus 1: {consistency_1}\")\n",
    "print(f\"Choice consistency for stimulus 2: {consistency_2}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1422c421",
   "metadata": {},
   "source": [
    "# Model repulsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "037faa75",
   "metadata": {},
   "source": [
    "# Define the rate parameter for the exponential distribution\n",
    "mu = 0.5  # Mean value for the prior\n",
    "s = 0.1   # Scale value for the prior\n",
    "# Set your noise levels\n",
    "sigma = 0.01  # Start with a small amount of internal noise # The encoding noise\n",
    "sigma_ext = 0.01  # Start with a small amount of external noise # The external noise after decoding\n",
    "delta = 0.05 \n",
    "num_videos = 30  # Assuming you have 30 videos per emotion\n",
    "# num_participants = 100  # Number of participants"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d74b50a6",
   "metadata": {},
   "source": [
    "# ATTENTION RE RUN mu and s definition otherwise they are theano form \n",
    "\n",
    "# Different levels of internal noise to simulate\n",
    "sigma_levels = [0.05, 0.1, 0.2,0.25,0.3, 0.4,0.45, 0.5]  # Different levels of internal noise\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "# Loop over different levels of internal noise\n",
    "for sigma in sigma_levels:\n",
    "    # Generate true emotion intensities\n",
    "    true_emotion_intensities = logistic.rvs(loc=mu, scale=s, size=num_videos)\n",
    "\n",
    "    # Simulate the encoding process with internal noise\n",
    "    encoded_responses = f(true_emotion_intensities, mu, s) + np.random.normal(0, sigma, size=num_videos)\n",
    "    \n",
    "    # Apply the decoding process\n",
    "    decoded_estimates = f_inv(encoded_responses, mu, s)\n",
    "\n",
    "    # Add external noise to the decoded estimates\n",
    "    observed_noisy_ratings = decoded_estimates + np.random.normal(0, sigma_ext, size=num_videos)\n",
    "    \n",
    "    # Calculate bias as the difference between observed noisy ratings and true emotion intensities\n",
    "    bias = observed_noisy_ratings - true_emotion_intensities\n",
    "    \n",
    "    # Store results\n",
    "    results.append((sigma, true_emotion_intensities, observed_noisy_ratings, bias))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43fcdf83",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for i, (sigma, true_intensities, noisy_ratings, bias) in enumerate(results):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.scatter(true_intensities, bias, alpha=0.2)\n",
    "    plt.title(f'Internal Noise Level: {sigma}')\n",
    "    plt.xlabel('True Emotion Intensity')\n",
    "    plt.ylabel('Bias (Noisy Rating - True Intensity)')\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0933b28",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for i, (sigma, true_intensities, noisy_ratings, bias) in enumerate(results):\n",
    "    # Sort the true intensities and corresponding biases\n",
    "    sorted_indices = np.argsort(true_intensities)\n",
    "    sorted_true_intensities = true_intensities[sorted_indices]\n",
    "    sorted_bias = bias[sorted_indices]\n",
    "\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.plot(sorted_true_intensities, sorted_bias, '-o', markersize=3, alpha=0.6)  # Line plot with small markers\n",
    "    plt.title(f'Internal Noise Level: {sigma}')\n",
    "    plt.xlabel('True Emotion Intensity')\n",
    "    plt.ylabel('Bias (Noisy Rating - True Intensity)')\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a2e9005",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "degree = 3  # Degree of the polynomial fit, adjust based on your data\n",
    "\n",
    "for i, (sigma, true_intensities, noisy_ratings, bias) in enumerate(results):\n",
    "    # Sorting the true_intensities and corresponding bias for plotting\n",
    "    sorted_indices = np.argsort(true_intensities)\n",
    "    sorted_intensities = true_intensities[sorted_indices]\n",
    "    sorted_bias = bias[sorted_indices]\n",
    "    \n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.scatter(sorted_intensities, sorted_bias, alpha=0.2)\n",
    "    \n",
    "    # Fit a polynomial regression for trend line\n",
    "    coeffs = np.polyfit(sorted_intensities, sorted_bias, degree)\n",
    "    polynomial = np.poly1d(coeffs)\n",
    "    trendline = polynomial(sorted_intensities)\n",
    "    \n",
    "    plt.plot(sorted_intensities, trendline, color='red')  # Trend line\n",
    "    \n",
    "    plt.title(f'Internal Noise Level: {sigma}')\n",
    "    plt.xlabel('True Emotion Intensity')\n",
    "    plt.ylabel('Bias (Noisy Rating - True Intensity)')\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f737c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
